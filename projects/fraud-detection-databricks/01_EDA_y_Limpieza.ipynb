{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d9f66c-f49d-4368-bc77-bc5193cda8a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente desde la tabla: workspace.default.creditcard\n+--------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+--------------------+-------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+-------------------+------+-----+\n|    Time|                 V1|                V2|                V3|                V4|                 V5|                V6|                V7|                 V8|                V9|               V10|               V11|               V12|               V13|               V14|               V15|                V16|               V17|              V18|                 V19|                V20|                 V21|               V22|                 V23|               V24|               V25|               V26|                V27|                V28|Amount|Class|\n+--------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+--------------------+-------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+-------------------+------+-----+\n|156285.0|  -1.77544230780845| -2.07957817525294| 0.953470988100582| -1.91662072493746| -0.634549193583271|  0.25855501135128| -1.03810926459446|  0.808690266184693|-0.569585017848712| -0.36721529879509|-0.813363131550551| 0.110627307630037| 0.586739371368621|-0.209785823413233|-0.379960511394044|  0.108314729060884| -1.04998846883454| 2.67866734358746|  -0.824483304185343|  0.247697912484599|   0.176570764893809|0.0851004430541731|    0.38528650260099| -1.07529463644384|-0.846894612494126|  1.16365641333224|-0.0366551224773531|-0.0920426154981305|212.09|    0|\n|156287.0|   1.95851366863488|-0.342583161217108|-0.710208261827209| 0.250127687399087| -0.214453316264448|  0.42731955823807|-0.968751821617827|  0.293556324269592|  1.06811790676532|-0.327383654924986| 0.607700706149556| 0.632339650516476| 0.326205308557038| -1.38940307087916| 0.283517652751805|   1.45504139996474|-0.257711359173847| 1.30926198004145|-1.19114506890491E-4|-0.0626252890349614|-0.00853682987262919|0.0626931255168073|   0.230950350141646|0.0136671179111893|-0.497700118176236| 0.357052570735827| 0.0011834448352718| -0.017822154600911| 16.01|    0|\n|156287.0|-0.0156282688474002|  1.20056058122318|-0.763822726541957| 0.211741062542994| -0.306215630502721| -1.89738547601724| 0.728335981620343|-0.0016367781643406|-0.127026363810451| -1.07281397100173| 0.144214306145098|0.0645168945338034|-0.369082484936996|-0.562481455597219| 0.632600139309327|-0.0061472222527084| 0.993505237480418|0.491661240241985|  -0.599636677092595| -0.310076659382954|   0.388358079842868|  1.07928792120935|   0.059994774200255| 0.846432265779894|-0.399821939088562|-0.217625097871509| -0.183098038883602|-0.0509051265209503| 42.81|    0|\n|156287.0|   1.45689449272835|-0.914980683644952| -2.07767930736329| 0.502258927079729|-0.0124940055204105|-0.460443139047721| 0.227467934319062|-0.0192379452778815| 0.912834294374093|-0.828108510760606|  1.09251118164813| 0.272478453913947| -1.49078337736287| -1.08004313500442| -0.55039963217029|  0.277981182172934|  1.06748794212811|0.723824995133043|   0.244905221391829|  0.314703321008401| -0.0532488647228351|-0.627698068426802|-0.00314903865002086| 0.553364616735254|-0.236979785052784|-0.162987723555769|-0.0697632682120923| 0.0224742458161661|278.21|    0|\n|156287.0|  0.170271579886142|  1.70049171512259| -2.74528991974939|-0.175250358855855|   1.49634394524223|  -1.5717982564946|   1.1358138498951| 0.0967481635328302| -0.45474460860308| -1.58784944055874|  -0.4099212727155|-0.516500948303893|-0.395757111906952| -2.23899980311193|-0.488930301123035|  0.494953143421768|  2.19010982653621|0.666852131145846|  -0.685959346802317| -0.130555203650268|  0.0239434704473009|  0.21685770902445| -0.0846943556342075| 0.413838891368901| -0.23439578705824| 0.543921393806212|  0.116156220375571| 0.0531182839848048|  0.76|    0|\n+--------+-------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-----------------+--------------------+-------------------+--------------------+------------------+--------------------+------------------+------------------+------------------+-------------------+-------------------+------+-----+\nonly showing top 5 rows\nEsquema (Schema) del DataFrame:\nroot\n |-- Time: double (nullable = true)\n |-- V1: double (nullable = true)\n |-- V2: double (nullable = true)\n |-- V3: double (nullable = true)\n |-- V4: double (nullable = true)\n |-- V5: double (nullable = true)\n |-- V6: double (nullable = true)\n |-- V7: double (nullable = true)\n |-- V8: double (nullable = true)\n |-- V9: double (nullable = true)\n |-- V10: double (nullable = true)\n |-- V11: double (nullable = true)\n |-- V12: double (nullable = true)\n |-- V13: double (nullable = true)\n |-- V14: double (nullable = true)\n |-- V15: double (nullable = true)\n |-- V16: double (nullable = true)\n |-- V17: double (nullable = true)\n |-- V18: double (nullable = true)\n |-- V19: double (nullable = true)\n |-- V20: double (nullable = true)\n |-- V21: double (nullable = true)\n |-- V22: double (nullable = true)\n |-- V23: double (nullable = true)\n |-- V24: double (nullable = true)\n |-- V25: double (nullable = true)\n |-- V26: double (nullable = true)\n |-- V27: double (nullable = true)\n |-- V28: double (nullable = true)\n |-- Amount: double (nullable = true)\n |-- Class: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# --- Fase 2: Carga de Datos ---\n",
    "# (Inicio de 01_EDA_y_Limpieza)\n",
    "\n",
    "# Nombre completo de la tabla que creaste en Unity Catalog\n",
    "table_name = \"workspace.default.creditcard\"\n",
    "\n",
    "# Cargamos los datos en un Spark DataFrame\n",
    "try:\n",
    "    df = spark.read.table(table_name)\n",
    "    \n",
    "    # Mostremos las primeras 5 filas para verificar\n",
    "    print(f\"Datos cargados exitosamente desde la tabla: {table_name}\")\n",
    "    df.show(5)\n",
    "\n",
    "    # Imprimamos el esquema que Databricks infirió\n",
    "    print(\"Esquema (Schema) del DataFrame:\")\n",
    "    df.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer la tabla: {e}\")\n",
    "    print(\"Verifica que el nombre del catálogo y la tabla ('workspace.default.creditcard') sean correctos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18b2513f-beae-4143-ad1a-7e66f238ffa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de Clases (0 = Legítima, 1 = Fraude):\n+-----+------+\n|Class| count|\n+-----+------+\n|    0|284315|\n|    1|   492|\n+-----+------+\n\n\n--- Resumen del Desbalance ---\nTotal de transacciones: 284,807\nTransacciones legítimas (0): 284,315\nTransacciones fraudulentas (1): 492\n\nPorcentaje de Fraude: 0.1727%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# 1. Agrupamos por la columna 'Class' y contamos las ocurrencias\n",
    "print(\"Distribución de Clases (0 = Legítima, 1 = Fraude):\")\n",
    "class_distribution = df.groupBy(\"Class\").count()\n",
    "class_distribution.show()\n",
    "\n",
    "# 2. Calculamos los conteos totales y de fraude para el porcentaje\n",
    "try:\n",
    "    total_count = df.count()\n",
    "    fraud_count = df.filter(col(\"Class\") == 1).count()\n",
    "    legit_count = df.filter(col(\"Class\") == 0).count()\n",
    "\n",
    "    # 3. Calculamos el porcentaje\n",
    "    percent_fraud = (fraud_count / total_count) * 100\n",
    "\n",
    "    print(f\"\\n--- Resumen del Desbalance ---\")\n",
    "    print(f\"Total de transacciones: {total_count:,}\")\n",
    "    print(f\"Transacciones legítimas (0): {legit_count:,}\")\n",
    "    print(f\"Transacciones fraudulentas (1): {fraud_count}\")\n",
    "    print(f\"\\nPorcentaje de Fraude: {percent_fraud:.4f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al calcular el desbalance: {e}\")\n",
    "    print(\"Asegúrate de que la columna 'Class' exista en tu DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bace269-3ed3-4eed-8e10-fb0cc14f5502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de características a ensamblar: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\nTotal: 29 características\n\n--- ¡Preprocesamiento Completo! ---\nDatos listos para el modelado (columna 'features' y 'Class'):\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Class|\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n|[-0.6942411021638889,-0.044074847193551966,1.6727705625425193,0.973363805557966,-0.24511615322164246,0.3470673358576208,0.1936785982994623,0.08263713433481515,0.33112720188981876,0.08338539885285726,-0.5404060870051803,-0.6182946323291787,-0.9960971732491681,-0.32460961644903336,1.6040110229374274,-0.536831926068152,0.24486302414440686,0.030769878583234905,0.49628115539005835,0.3261174434913665,-0.02492332120662145,0.3828537662094798,-0.17691102375613202,0.11050672655740297,0.24658501006984707,-0.39216974306341384,0.3308910417432028,-0.06378103872482459,0.24496383331670502] |0    |\n|[0.608495259431098,0.1611756369266395,0.10979690934882849,0.31652237106795644,0.04348327570891679,-0.06181985742147788,-0.063700097913428,0.07125335796867181,-0.2324937807792634,-0.15334935939751942,1.5800000757024917,1.066086699372764,0.49141734116292335,-0.1499822185217231,0.6943591996604319,0.5294328243583323,-0.1351697317972667,-0.21876219825788867,-0.17908573490829827,-0.08961070531227908,-0.30737626492223263,-0.8800752094067619,0.16220089886855976,-0.5611295646957848,0.32069333790440707,0.2610690170938659,-0.02225563911549297,0.04460743936822104,-0.3424739398649144]   |0    |\n|[-0.6934992452238228,-0.811576401523055,1.1694664397320733,0.26823082294214534,-0.36457114580409156,1.351451213371488,0.6397745147279924,0.20737236543359464,-1.3786729310585075,0.1906992790138803,0.6118286359340631,0.06613650257222697,0.7206985870825081,-0.1731135849307112,2.562901685602999,-3.298229581937309,1.3068655851353688,-0.14478973730846778,-2.778555972545103,0.6809737760183283,0.33763110343236136,1.0633564043163728,1.456317189039573,-1.13809014045685,-0.6285356170752578,-0.2884462456250831,-0.13713661493785625,-0.1810205093097717,1.1606838875569427]                 |0    |\n|[-0.49332403207741626,-0.11216922771730709,1.182514374861699,-0.6097255708019923,-0.007468867231422749,0.9361481886931923,0.19207030100296038,0.31601704471831615,-1.262500955785173,-0.05046786454609063,-0.22189122474824072,0.17837067456552827,0.5101678056040073,-0.3003599663021526,-0.6898361983181233,-1.2092938701642704,-0.8054432281407387,2.3453004047924275,-1.5142022650290956,-0.2698547516872068,-0.1474430378266678,0.00726689464375873,-0.30477601231876816,-1.941023731990898,1.2419015323822127,-0.46021653361477394,0.15539593444957295,0.18618825967135022,0.14053400526590598]|0    |\n|[-0.5913287255806606,0.531540116582236,1.0214098823894193,0.28465490447934827,-0.29501491818228925,0.07199845677494839,0.4793014419091613,-0.22650983355730342,0.744324980360489,0.6916238180002339,-0.8061451706863962,0.5386257022751219,1.3522419776167212,-1.1680314634958793,0.19132313625856262,-0.5152042170828698,-0.27908029626933406,-0.04556892286021194,0.9870355642029597,0.5299378632216263,-0.012839195108414908,1.1000093400718374,-0.22012300956314837,0.233249678449826,-0.39520094794314836,1.0416094709933836,0.5436188420170627,0.6518147717866517,-0.0734032113879323]         |0    |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Definir la lista de columnas de características.\n",
    "# Excluimos 'Time' (débil) y 'Class' (nuestra etiqueta a predecir).\n",
    "feature_columns = [c for c in df.columns if c not in ['Time', 'Class']]\n",
    "\n",
    "# Imprimimos para verificar (debería ser V1...V28 y Amount)\n",
    "print(f\"Columnas de características a ensamblar: {feature_columns}\")\n",
    "print(f\"Total: {len(feature_columns)} características\") # Deberían ser 29\n",
    "\n",
    "# 2. Configurar el VectorAssembler\n",
    "# Entrada: Las 29 columnas\n",
    "# Salida: Una nueva columna llamada \"unscaled_features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"unscaled_features\"\n",
    ")\n",
    "\n",
    "# 3. Configurar el StandardScaler\n",
    "# Entrada: \"unscaled_features\"\n",
    "# Salida: \"features\" (el nombre estándar que espera pyspark.ml)\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"unscaled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,  # Escalar a desviación estándar 1\n",
    "    withMean=True  # Centrar datos en media 0\n",
    ")\n",
    "\n",
    "# 4. Definir el Pipeline\n",
    "# Un pipeline encadena estas etapas en orden\n",
    "preprocessing_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# 5. \"Entrenar\" el pipeline\n",
    "# El pipeline \"aprende\" las estadísticas (media, std) de los datos\n",
    "pipeline_model = preprocessing_pipeline.fit(df)\n",
    "\n",
    "# 6. Transformar los datos\n",
    "# Aplicamos la transformación a nuestro DataFrame\n",
    "df_processed = pipeline_model.transform(df)\n",
    "\n",
    "# 7. Seleccionamos solo las columnas que necesitamos para el modelado\n",
    "# 'features' (el vector escalado) y 'Class' (la etiqueta)\n",
    "df_final = df_processed.select(\"features\", \"Class\")\n",
    "\n",
    "print(\"\\n--- ¡Preprocesamiento Completo! ---\")\n",
    "print(\"Datos listos para el modelado (columna 'features' y 'Class'):\")\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb0959c-530b-45b1-8734-34eaf28957ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- División Estratificada Completa ---\nTotal original: 284807\nTotal en Entrenamiento (Train): 227813 (~79.99%)\nTotal en Prueba (Test): 54319 (~19.07%)\n\n--- Verificación del Test Set (¡Debe estar desbalanceado!) ---\n+-----+-----+\n|Class|count|\n+-----+-----+\n|    0|54237|\n|    1|   82|\n+-----+-----+\n\n\n--- Verificación del Train Set (¡Aún desbalanceado!) ---\n+-----+------+\n|Class| count|\n+-----+------+\n|    0|227409|\n|    1|   404|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# --- Fase 3: División de Datos y Prevención de Fuga ---\n",
    "\n",
    "# 1. Definir las fracciones de muestreo para la división estratificada\n",
    "# Queremos 80% para entrenamiento y 20% para prueba\n",
    "fractions = {\n",
    "    0: 0.8,  # 80% de la clase 0 (legítima)\n",
    "    1: 0.8   # 80% de la clase 1 (fraude)\n",
    "}\n",
    "\n",
    "# 2. Crear el conjunto de entrenamiento (train_data)\n",
    "# Usamos sampleBy para tomar el 80% de cada clase\n",
    "train_data = df_final.sampleBy(\"Class\", fractions, seed=42)\n",
    "\n",
    "# 3. Crear el conjunto de prueba (test_data)\n",
    "# Usamos subtract() para obtener todo lo que NO estaba en train_data\n",
    "test_data = df_final.subtract(train_data)\n",
    "\n",
    "# 4. Verificar la división\n",
    "print(\"--- División Estratificada Completa ---\")\n",
    "total_count = df_final.count()\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "print(f\"Total original: {total_count}\")\n",
    "print(f\"Total en Entrenamiento (Train): {train_count} (~{train_count/total_count:.2%})\")\n",
    "print(f\"Total en Prueba (Test): {test_count} (~{test_count/total_count:.2%})\")\n",
    "\n",
    "# 5. Verificación más importante: el desbalance en el Test Set\n",
    "print(\"\\n--- Verificación del Test Set (¡Debe estar desbalanceado!) ---\")\n",
    "test_data.groupBy(\"Class\").count().show()\n",
    "\n",
    "print(\"\\n--- Verificación del Train Set (¡Aún desbalanceado!) ---\")\n",
    "train_data.groupBy(\"Class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197c60c0-2dfe-47b4-a1f5-f4c3fa0e36b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /local_disk0/.ephemeral_nfs/envs/pythonEnv-04f009ff-7269-41cc-a190-9a17667f5f91/lib/python3.12/site-packages (0.14.0)\nRequirement already satisfied: numpy<3,>=1.25.2 in /databricks/python3/lib/python3.12/site-packages (from imbalanced-learn) (2.1.3)\nRequirement already satisfied: scipy<2,>=1.11.4 in /databricks/python3/lib/python3.12/site-packages (from imbalanced-learn) (1.15.1)\nRequirement already satisfied: scikit-learn<2,>=1.4.2 in /databricks/python3/lib/python3.12/site-packages (from imbalanced-learn) (1.6.1)\nRequirement already satisfied: joblib<2,>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\nIniciando el proceso de SMOTE...\nPaso 1/5: Convirtiendo el 'train_data' de Spark a Pandas...\nPaso 2/5: Datos convertidos. 227813 filas en Pandas.\nPaso 3/5: Datos de 'features' (X) y 'Class' (y) separados.\nForma de X_train (antes de SMOTE): (227813, 29)\nForma de y_train (antes de SMOTE): (227813,)\nPaso 4/5: Aplicando SMOTE... (Esto puede tardar un momento)\nPaso 5/5: ¡SMOTE completado!\n--- Verificación Post-SMOTE (en NumPy) ---\nForma de X (balanceado): (454818, 29)\nConteo de clases en 'y' (balanceado):\nClass\n0    227409\n1    227409\nName: count, dtype: int64\n\nConvirtiendo datos balanceados de vuelta a Spark DataFrame...\n\n--- ¡Conversión a Spark DF Completa! ---\nVerificación del DataFrame 'train_balanced_data' (en Spark):\n+-----+------+\n|Class| count|\n+-----+------+\n|    0|227409|\n|    1|227409|\n+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# --- Fase 3: Aplicación de SMOTE (Solo en Train) ---\n",
    "\n",
    "# 1. Instalar la biblioteca (requerido en Databricks)\n",
    "# La celda puede tardar un momento en ejecutar esto\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "# 2. Importar bibliotecas necesarias\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "print(\"\\nIniciando el proceso de SMOTE...\")\n",
    "print(\"Paso 1/5: Convirtiendo el 'train_data' de Spark a Pandas...\")\n",
    "\n",
    "# 3. Exportar a Pandas (¡El cuello de botella!)\n",
    "try:\n",
    "    train_pd = train_data.toPandas()\n",
    "except Exception as e:\n",
    "    print(f\"Error al convertir a Pandas: {e}\")\n",
    "    print(\"Esto puede fallar si el driver node no tiene suficiente memoria.\")\n",
    "    raise e\n",
    "\n",
    "print(f\"Paso 2/5: Datos convertidos. {len(train_pd)} filas en Pandas.\")\n",
    "\n",
    "# 4. Preparar los datos para SMOTE (formato NumPy)\n",
    "# 'y' son las etiquetas\n",
    "y_train_pd = train_pd['Class']\n",
    "\n",
    "# 'X' son las características. Debemos convertir la columna 'features'\n",
    "# (que contiene Vectores de Spark) a un array 2D de NumPy.\n",
    "X_train_pd = np.stack(train_pd['features'].apply(lambda x: x.toArray()))\n",
    "\n",
    "print(f\"Paso 3/5: Datos de 'features' (X) y 'Class' (y) separados.\")\n",
    "print(f\"Forma de X_train (antes de SMOTE): {X_train_pd.shape}\")\n",
    "print(f\"Forma de y_train (antes de SMOTE): {y_train_pd.shape}\")\n",
    "\n",
    "# 5. Configurar y aplicar SMOTE\n",
    "# Usamos k_neighbors=5 (default). Esto está bien porque tenemos > 5 fraudes.\n",
    "smote = SMOTE(random_state=42)\n",
    "print(\"Paso 4/5: Aplicando SMOTE... (Esto puede tardar un momento)\")\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_pd, y_train_pd)\n",
    "\n",
    "print(f\"Paso 5/5: ¡SMOTE completado!\")\n",
    "print(f\"--- Verificación Post-SMOTE (en NumPy) ---\")\n",
    "print(f\"Forma de X (balanceado): {X_resampled.shape}\")\n",
    "print(f\"Conteo de clases en 'y' (balanceado):\")\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "# 6. Convertir los datos balanceados (NumPy) de nuevo a un Spark DataFrame\n",
    "print(\"\\nConvirtiendo datos balanceados de vuelta a Spark DataFrame...\")\n",
    "\n",
    "# Creamos una lista de tuplas (Vectors.dense(features), Class)\n",
    "data_tuples = [\n",
    "    (Vectors.dense(row), int(y)) for row, y in zip(X_resampled, y_resampled)\n",
    "]\n",
    "\n",
    "# Creamos el DataFrame 'train_balanced_data'\n",
    "train_balanced_data = spark.createDataFrame(\n",
    "    data_tuples, \n",
    "    [\"features\", \"Class\"] # Mantenemos los nombres de columna\n",
    ")\n",
    "\n",
    "print(\"\\n--- ¡Conversión a Spark DF Completa! ---\")\n",
    "print(\"Verificación del DataFrame 'train_balanced_data' (en Spark):\")\n",
    "train_balanced_data.groupBy(\"Class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa45ab76-10c2-4a08-a571-1e02c77db0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluadores listos (AUC-PR, Precision, Recall).\nIniciando ejecución (Run) de Regresión Logística...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/10 07:34:04 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/11/10 07:34:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-04f009ff-7269-41cc-a190-9a/tmp9tobqnsj/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2025/11/10 07:34:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- ¡Ejecución de Regresión Logística Completa! ---\n  Resultados (en test_data):\n  AUC-PR: 0.6320\n  Precision (Clase 1): 0.4012\n  Recall (Clase 1): 0.8171\n"
     ]
    }
   ],
   "source": [
    "# --- Fase 4: Entrenamiento de Modelos y Seguimiento ---\n",
    "\n",
    "import mlflow\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# --- 1. Configurar los Evaluadores ---\n",
    "# Debemos decirle a Spark CÓMO medir el rendimiento.\n",
    "\n",
    "# Evaluador principal: AUC-PR (perfecto para desbalance)\n",
    "# Usamos BinaryClassificationEvaluator para esto.\n",
    "evaluator_pr = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Class\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderPR\"  # ¡La métrica clave!\n",
    ")\n",
    "\n",
    "# Evaluadores secundarios: Precision y Recall\n",
    "# Usamos MulticlassClassificationEvaluator (funciona para binario)\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Class\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"precisionByLabel\",\n",
    "    metricLabel=1 # Nos importa la precisión de la clase 1 (fraude)\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Class\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"recallByLabel\",\n",
    "    metricLabel=1 # Nos importa el recall de la clase 1 (fraude)\n",
    ")\n",
    "\n",
    "print(\"Evaluadores listos (AUC-PR, Precision, Recall).\")\n",
    "\n",
    "# --- 2. Iniciar el Experimento de MLflow ---\n",
    "# Nombramos el experimento para agrupar todas nuestras ejecuciones\n",
    "mlflow.set_experiment(\"/fraud_detection_project\")\n",
    "\n",
    "# --- 3. Ejecución del Modelo: Regresión Logística ---\n",
    "print(\"Iniciando ejecución (Run) de Regresión Logística...\")\n",
    "\n",
    "try:\n",
    "    # Usamos 'with' para que MLflow inicie y termine el registro automáticamente\n",
    "    with mlflow.start_run(run_name=\"Logistic Regression\"):\n",
    "\n",
    "        # a. Definir el modelo y los parámetros\n",
    "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Class\", regParam=0.1)\n",
    "        \n",
    "        # b. Registrar Parámetros\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"regParam\", 0.1)\n",
    "        mlflow.log_param(\"notes\", \"Baseline model with SMOTE data\")\n",
    "\n",
    "        # c. Entrenar el modelo\n",
    "        # ¡Usamos los datos balanceados por SMOTE!\n",
    "        model = lr.fit(train_balanced_data)\n",
    "\n",
    "        # d. Generar Predicciones\n",
    "        # ¡Evaluamos en los datos de prueba desbalanceados!\n",
    "        predictions = model.transform(test_data)\n",
    "        \n",
    "        # e. Calcular y Registrar Métricas\n",
    "        auc_pr = evaluator_pr.evaluate(predictions)\n",
    "        precision = evaluator_precision.evaluate(predictions)\n",
    "        recall = evaluator_recall.evaluate(predictions)\n",
    "        \n",
    "        mlflow.log_metric(\"AUC_PR\", auc_pr)\n",
    "        mlflow.log_metric(\"Precision\", precision)\n",
    "        mlflow.log_metric(\"Recall\", recall)\n",
    "\n",
    "        # f. Registrar el Modelo\n",
    "        mlflow.spark.log_model(\n",
    "            model, \n",
    "            \"model\", \n",
    "            dfs_tmpdir=\"/Volumes/workspace/default/mlflow_staging\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- ¡Ejecución de Regresión Logística Completa! ---\")\n",
    "        print(f\"  Resultados (en test_data):\")\n",
    "        print(f\"  AUC-PR: {auc_pr:.4f}\")\n",
    "        print(f\"  Precision (Clase 1): {precision:.4f}\")\n",
    "        print(f\"  Recall (Clase 1): {recall:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la ejecución de MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39fe9bc-f3f4-4bf2-9fc9-04a6ad65f976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ejecución (Run) de Random Forest...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/10 07:35:46 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2025/11/10 07:35:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-04f009ff-7269-41cc-a190-9a/tmpfg0bozfe/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2025/11/10 07:35:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- ¡Ejecución de Random Forest Completa! ---\n  Resultados (en test_data):\n  AUC-PR: 0.6932\n  Precision (Clase 1): 0.2632\n  Recall (Clase 1): 0.8537\n"
     ]
    }
   ],
   "source": [
    "# --- Fase 4: Ejecución del Modelo 2 (Random Forest - CORREGIDO) ---\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Iniciando ejecución (Run) de Random Forest...\")\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"Random Forest\"):\n",
    "\n",
    "        # a. Definir el modelo y los parámetros\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\", \n",
    "            labelCol=\"Class\",\n",
    "            numTrees=100,\n",
    "            maxDepth=5,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # b. Registrar Parámetros\n",
    "        mlflow.log_param(\"model_type\", \"RandomForestClassifier\")\n",
    "        mlflow.log_param(\"numTrees\", 100)\n",
    "        mlflow.log_param(\"maxDepth\", 5)\n",
    "        mlflow.log_param(\"notes\", \"Modelo de ensamble con SMOTE data\")\n",
    "\n",
    "        # c. Entrenar el modelo\n",
    "        model = rf.fit(train_balanced_data)\n",
    "\n",
    "        # d. Generar Predicciones\n",
    "        predictions = model.transform(test_data)\n",
    "        \n",
    "        # e. Calcular y Registrar Métricas\n",
    "        auc_pr = evaluator_pr.evaluate(predictions)\n",
    "        precision = evaluator_precision.evaluate(predictions)\n",
    "        recall = evaluator_recall.evaluate(predictions)\n",
    "        \n",
    "        mlflow.log_metric(\"AUC_PR\", auc_pr)\n",
    "        mlflow.log_metric(\"Precision\", precision)\n",
    "        mlflow.log_metric(\"Recall\", recall)\n",
    "        \n",
    "        # f. Registrar el Modelo (con Firma y el tmpdir)\n",
    "        mlflow.spark.log_model(\n",
    "            model, \n",
    "            \"model\", \n",
    "            dfs_tmpdir=\"/Volumes/workspace/default/mlflow_staging\",\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- ¡Ejecución de Random Forest Completa! ---\")\n",
    "        print(f\"  Resultados (en test_data):\")\n",
    "        print(f\"  AUC-PR: {auc_pr:.4f}\")\n",
    "        print(f\"  Precision (Clase 1): {precision:.4f}\")\n",
    "        print(f\"  Recall (Clase 1): {recall:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la ejecución de MLflow: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_EDA_y_Limpieza",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}